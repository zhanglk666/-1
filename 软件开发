 import numpy as np
 import torch
 import torch.nn as nn
 class Rj(nn.Module):
     def __init__(self):
         super().__init__()
     def forward(self,input):
         output=input+1
         return output

 rj=Rj()
 x=torch.tensor(1)
 print(rj.forward(x))




import torch
 import numpy as np
 import torch.nn as nn
 
 class Net(nn.Module):
     def __init__(self,input_size,hidden_size,output_size) -> None:
         super().__init__()
         self.fc1 = nn.Linear(input_size,hidden_size)
         self.sigmoid = torch.nn.Sigmoid() #torch.sigmoid
         self.fc2 = nn.Linear(hidden_size,output_size) #/
 
     def forward(self,x,w1,w2):
         self.fc1.weight.data = w1
         self.fc1.bias.data = torch.Tensor([0])
         h = self.fc1(x)
         h = self.sigmoid(h)
 
         self.fc2.weight.data = w2
         self.fc2.bias.data = torch.Tensor([0])
         o = self.fc2(h)
         o = self.sigmoid(o)
         return o
 
 if __name__=='__main__':
     x = torch.tensor([0.5, 0.3])
     w1 = torch.tensor([[0.2, 0.5], [-0.4, 0.6]])
     w2 = torch.tensor([[0.1, -0.3], [-0.5, 0.8]])
 
     net = Net(2,2,2)
     output = net(x,w1,w2)
     # net.forward(x,w1,w2)
     print('最终的输出值为：',output)



 import torch
 device=torch.device('cuda')
 my_tensor=torch.tensor([[1,2,3],[4,5,6]],dtype=torch.float32,device=device,requires_grad=True)
 print(my_tensor)
 print(my_tensor.dtype)
 print(my_tensor.device)
 print(my_tensor.shape)
 print(my_tensor.requires_grad)
 x = torch.empty(size = (3,3))
 x = torch.zeros((3,3))
 x = torch.rand((3,3))
 x = torch .ones((3, 3))
 x = torch.eye(5,5)
 x = torch.arange(start=0, end=5, step=1)
 x = torch.linspace(start=0.1, end=1, steps=10)
 x = torch.empty(size=(1, 5)).normal_(mean=0, std=1)
 x = torch.empty(size=(1,5)).uniform_(0,1)
 x = torch.diag(torch.ones(3))

 tensor = torch.arange(4)
 print(tensor.bool())
 print(tensor.short())
 print(tensor.long())
